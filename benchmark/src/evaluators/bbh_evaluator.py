"""
BBH Evaluator

This module provides a standalone evaluator for Big-Bench Hard (BBH) problems.
"""

import re
import time
import json
from typing import Dict, Any, Tuple
from pathlib import Path
import uuid

from langsmith.evaluation import RunEvaluator
from langsmith.schemas import Run

class BBHEvaluator:
    """
    Evaluator for Big-Bench Hard (BBH) problems.

    This evaluator handles diverse BBH tasks, extracting answers from model responses and comparing them with expected outputs.
    """

    def __init__(self, name: str = "bbh", config: Dict[str, Any] = None):
        """
        Initialize the BBH Evaluator.

        Args:
            name: Name of the evaluator (default: "bbh")
            config: Configuration parameters
        """
        self.name = name
        self.config = config or {}

        # Set up paths
        self.data_path = config.get("data_path", f"benchmark/data/{name}_test.jsonl")
        self.log_path = config.get("log_path", f"benchmark/data/results/{name.upper()}")

        # Create log directory
        Path(self.log_path).mkdir(parents=True, exist_ok=True)

        # Initialize run evaluator
        self.run_evaluator = RunEvaluator()

    def extract_answer(self, text: str) -> str:
        """
        Extract the answer from model output text, expecting '<answer>...</answer>' tags first.

        Args:
            text: The model's output text

        Returns:
            The extracted answer (e.g., "(A)", "True", "] >")
        """
        text = text.strip()

        # Primary pattern: Content within <answer>...</answer> tags
        tag_pattern = r"<answer>\s*([\s\S]*?)\s*</answer>"
        match = re.search(tag_pattern, text, re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # Fallback: "Final Answer: <answer>"
        final_answer_pattern = r"Final Answer:\s*(.+)"
        match = re.search(final_answer_pattern, text, re.DOTALL)
        if match:
            return match.group(1).strip()

        # Fallback: Look for multiple-choice options (e.g., (A), A, [A])
        option_pattern = r"\([A-Z]\)|[A-Z]\b|\[[A-Z]\]"
        matches = re.findall(option_pattern, text, re.DOTALL)
        if matches:
            last_match = matches[-1]
            # Normalize to (A) format
            if not last_match.startswith("("):
                last_match = f"({last_match[-1]})"
            return last_match.strip()

        # Fallback: Look for boolean values
        boolean_pattern = r"\b(True|False)\b"
        boolean_matches = re.findall(boolean_pattern, text, re.DOTALL)
        if boolean_matches:
            return boolean_matches[-1].strip()

        # Fallback: Look for sequence completions (e.g., "> ) }", "] ] ]")
        sequence_pattern = r"([>\]\}\)\[]+\s*)+"
        sequence_matches = re.findall(sequence_pattern, text, re.DOTALL)
        if sequence_matches:
            return sequence_matches[-1].strip()

        # Fallback: Last non-empty line
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        if lines:
            return lines[-1]

        # Final fallback: Return stripped text
        return text.strip()

    def normalize_answer(self, answer: str) -> str:
        """
        Normalize the answer to handle minor formatting variations.

        Args:
            answer: The extracted answer

        Returns:
            Normalized answer
        """
        answer = answer.strip()
        # Normalize multiple-choice answers (e.g., "A" to "(A)", "[A]" to "(A)")
        if re.match(r"^[A-Z]$", answer):
            return f"({answer})"
        if re.match(r"^\[[A-Z]\]$", answer):
            return f"({answer[1]})"
        # Normalize sequence answers by collapsing extra spaces
        if re.match(r"^[>\]\}\)\[]+\s*[>\]\}\)\[]*\s*$", answer):
            return " ".join(answer.split())
        # Normalize boolean answers to title case
        if answer.lower() in ["true", "false"]:
            return answer.title()
        return answer

    def calculate_score(self, expected_output: str, prediction: str, problem_id: str = "") -> Tuple[float, str, str]:
        """
        Calculate score by comparing expected and predicted answers.

        Args:
            expected_output: The expected answer (solution)
            prediction: The model's raw prediction
            problem_id: The ID of the problem (used to identify word sorting tasks)

        Returns:
            Tuple of (score, extracted_answer, message) where score is 1.0 for correct, 0.0 for incorrect
        """
        extracted_answer = self.extract_answer(prediction)
        normalized_answer = self.normalize_answer(extracted_answer)
        normalized_expected = self.normalize_answer(expected_output.strip())

        # Check if this is a word sorting task
        is_word_sorting = "word_sorting" in problem_id.lower()

        if is_word_sorting:
            # For word sorting tasks, compare words as sets (order doesn't matter)
            predicted_words = set(normalized_answer.lower().split())
            expected_words = set(normalized_expected.lower().split())
            if predicted_words == expected_words:
                return 1.0, extracted_answer, "Correct"
            else:
                error_message = f"Incorrect word sorting: Expected words '{normalized_expected}', got '{normalized_answer}'"
                with open(f"{self.log_path}/error.log", "a", encoding="utf-8") as log_file:
                    log_file.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {error_message}\n")
                return 0.0, extracted_answer, error_message
        else:
            # For other tasks, use exact string comparison
            if normalized_answer == normalized_expected:
                return 1.0, extracted_answer, "Correct"

            error_message = f"Incorrect: Expected '{normalized_expected}', got '{normalized_answer}'"
            with open(f"{self.log_path}/error.log", "a", encoding="utf-8") as log_file:
                log_file.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {error_message}\n")
            return 0.0, extracted_answer, error_message

    def create_run(self, problem: Dict[str, Any], final_answer: str, extracted_answer: str, score: float, message: str) -> Run:
        """
        Create a LangSmith run for evaluation.

        Args:
            problem: The problem dictionary
            final_answer: The raw final answer from the model
            extracted_answer: The extracted answer
            score: The score (0.0 or 1.0)
            message: Evaluation message

        Returns:
            A LangSmith Run object
        """
        return Run(
            id=str(uuid.uuid4()),
            name=f"{self.name.upper()}_Evaluation",
            inputs={"problem": problem["problem"], "task_id": problem["id"]},
            outputs={
                "prediction": final_answer,
                "extracted_answer": extracted_answer,
                "expected": problem["solution"],
                "score": score,
                "message": message,
                "passed": score == 1.0,
            },
            run_type="evaluation",
            start_time=time.strftime("%Y-%m-%dT%H:%M:%SZ"),
            trace_id=str(uuid.uuid4()),
        )

    def evaluate(self, problem: Dict[str, Any], run_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate a BBH problem given the agent's response.

        Args:
            problem: The problem dictionary with "problem" and "solution" keys
            run_result: The result from running the agent system, including the final answer

        Returns:
            Evaluation results dictionary
        """
        # Extract the final answer
        final_answer = run_result.get("final_answer", "")
        if not final_answer:
            final_answer = run_result.get("content", "") if isinstance(run_result, dict) else str(run_result)

        # Calculate score using the solution key, passing problem_id
        score, extracted_answer, message = self.calculate_score(problem["solution"], final_answer, problem["id"])

        # Create LangSmith run
        run = self.create_run(problem, final_answer, extracted_answer, score, message)
        run_evaluation = self.run_evaluator.evaluate_run(run=run)

        return {
            "final_answer": final_answer,
            "extracted_answer": extracted_answer,
            "score": score,
            "message": message,
            "run_evaluation": run_evaluation,
        }

    def run_benchmark(self, agent, max_problems: int = None):
        """
        Run the BBH benchmark on the provided agent.

        Args:
            agent: The agent to evaluate (must have a run_agent method)
            max_problems: Maximum number of problems to evaluate (None for all)

        Returns:
            Evaluation results dictionary
        """
        results = []
        with open(self.data_path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                if max_problems and i >= max_problems:
                    break
                problem = json.loads(line.strip())
                try:
                    # Run agent on problem
                    agent_response = agent.run_agent(problem, problem_type="bbh")
                    # Evaluate
                    result = self.evaluate(problem, agent_response)
                    results.append(result)
                except Exception as e:
                    error_message = f"Error evaluating problem {problem['task_id']}: {str(e)}"
                    with open(f"{self.log_path}/error.log", "a", encoding="utf-8") as log_file:
                        log_file.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {error_message}\n")
                    results.append({
                        "final_answer": "",
                        "extracted_answer": "",
                        "score": 0.0,
                        "message": error_message,
                        "run_evaluation": None,
                    })

        # Calculate overall accuracy
        total_score = sum(r["score"] for r in results)
        accuracy = total_score / len(results) if results else 0.0

        return {
            "results": results,
            "accuracy": accuracy,
            "total_problems": len(results),
            "correct": int(total_score),
        }